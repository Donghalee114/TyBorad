from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import json, time, re
from typing import List

# ğŸ”¹ ë³‘í•©ëœ LoRA ëª¨ë¸ ë””ë ‰í† ë¦¬
model_dir = "/workspace_sync/lora_finetune/lora_script/merged_qwen15b"

# ğŸ”¹ tokenizerëŠ” ì›ë³¸ Qwen ëª¨ë¸ì—ì„œ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(
    "Qwen/Qwen2.5-1.5B-Instruct",
    trust_remote_code=True
)

# ğŸ”¹ ë³‘í•©ëœ ëª¨ë¸ ë¡œë”©
model = AutoModelForCausalLM.from_pretrained(
    model_dir,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True,
    local_files_only=True
)
model.eval()

# ğŸ”¹ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ë¡œë”©
with open("/workspace_sync/ELSA_benchmark_v0.1_Cleaned.json", "r", encoding="utf-8") as f:
    dataset = json.load(f)

# ğŸ”¹ ì •ë‹µ ì¶”ì¶œ í•¨ìˆ˜ (ë§ˆì§€ë§‰ ìˆ«ìë§Œ ì¶”ì¶œ)
def extract_answer(output_text: str) -> str:
    output_text = output_text.strip()
    matches = re.findall(r"\b[1-4]\b", output_text)
    return matches[-1] if matches else None

# ğŸ”¹ ëª¨ë¸ ì§ˆì˜ í•¨ìˆ˜
def ask_custom_model(question_text: str, choices: List[str]) -> str:
    prompt = f"""ë‹¤ìŒ ê°ê´€ì‹ ë¬¸ì œë¥¼ ì½ê³  ì •ë‹µ ë²ˆí˜¸ë§Œ ìˆ«ìë¡œ ì¶œë ¥í•˜ì„¸ìš”. (ì˜ˆ: 3)

ë¬¸ì œ: {question_text}

ë³´ê¸°:
"""
    for i, choice in enumerate(choices, start=1):
        prompt += f"{i}. {choice}\n"
    prompt += "\nì •ë‹µ:"

    print("\n=== INPUT PROMPT ===")
    print(prompt)

    # ëª¨ë¸ ì…ë ¥ ë° ìƒì„±
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=10,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.1,
            eos_token_id=tokenizer.eos_token_id,
        )

    output_text = tokenizer.decode(outputs[0], skip_special_tokens=False).strip()
    print(f"\n=== Model Output ===\n{output_text}\n")

    # ì •ë‹µ ì¶”ì¶œ
    predicted = extract_answer(output_text)
    if not predicted:
        print(f"âš ï¸ ì •ë‹µ ì¶”ì¶œ ì‹¤íŒ¨ â†’ ì›ë³¸ ì‘ë‹µ:\n{output_text}")
    return predicted

# ğŸ”¹ ì •í™•ë„ í‰ê°€ í•¨ìˆ˜
def evaluate_custom_model(model_label: str):
    correct, total = 0, 0
    print(f"\n== í‰ê°€ ì‹œì‘: {model_label} ==")

    for item in dataset:
        q, c, a = item["question"], item["choices"], item["answer"]
        pred = ask_custom_model(q, c)
        if pred == a:
            correct += 1
        total += 1
        time.sleep(0.05)

    acc = correct / total * 100
    print(f"\n== {model_label} ì •í™•ë„: {acc:.2f}% ({correct}/{total}) ==")

# ğŸ”¹ ì‹¤í–‰
if __name__ == "__main__":
    evaluate_custom_model("Qwen2.5-1.5B ë³‘í•© LoRA ëª¨ë¸")
